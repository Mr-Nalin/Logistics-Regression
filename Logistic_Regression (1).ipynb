{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression Theory"
      ],
      "metadata": {
        "id": "DTaRpksh6yo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1 What is Logistic Regression, and how does it differ from Linear Regression?"
      ],
      "metadata": {
        "id": "6SSfsnEk6ymd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Logistic Regression is a statistical model used for binary classification problems — that is, when the outcome variable can take only two possible values (e.g., yes/no, 0/1, spam/not spam).\n",
        "\n",
        "It models the probability that a given input belongs to a particular category.\n",
        "\n",
        "Instead of predicting a continuous value, it predicts the probability of the input belonging to the positive class.\n",
        "\n",
        "To do this, it uses the logistic function (sigmoid function) to map any real-valued number into a value between 0 and 1, which can be interpreted as a probability.\n",
        "\n",
        "Mathematically, logistic regression models:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        ")\n",
        "P(Y=1∣X)=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "where\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "P(Y=1∣X) is the probability that the outcome\n",
        "𝑌\n",
        "Y equals 1 given input features\n",
        "𝑋\n",
        "X.\n",
        "\n",
        "What is Linear Regression?\n",
        "Linear Regression is used for predicting continuous numerical values.\n",
        "\n",
        "It models the relationship between a dependent variable and one or more independent variables by fitting a linear equation.\n",
        "\n",
        "The prediction is a continuous value, like price, temperature, or weight.\n",
        "\n",
        "Mathematically, linear regression models:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜖\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " +ϵ\n",
        "where\n",
        "𝑌\n",
        "Y is the continuous outcome and\n",
        "𝜖\n",
        "ϵ is the error term.\n",
        "\n",
        "Key Differences\n",
        "Aspect\tLogistic Regression\tLinear Regression\n",
        "Output\tProbability (0 to 1) → classification\tContinuous value\n",
        "Use case\tClassification (binary or sometimes multi-class)\tRegression (predicting continuous values)\n",
        "Model function\tLogistic/sigmoid function\tLinear function\n",
        "Loss function\tCross-entropy (log loss)\tMean squared error (MSE)\n",
        "Interpretation\tOdds and probabilities\tDirect prediction of numerical outcome\n",
        "Assumptions\tLinear relationship between input and log-odds\tLinear relationship between input and output\n"
      ],
      "metadata": {
        "id": "tH6gzDbE6yjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 What is the mathematical equation of Logistic Regression?"
      ],
      "metadata": {
        "id": "YoG6xSVS6ygs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->The mathematical equation of Logistic Regression models the probability that a given input\n",
        "𝑥\n",
        "x belongs to a particular class (usually class 1). The key idea is to model the log-odds (logit) of the probability as a linear function of the input features.\n",
        "\n",
        "Logistic Regression Equation\n",
        "For a binary classification problem, the probability that the output\n",
        "𝑦\n",
        "y is 1 given input\n",
        "𝑥\n",
        "x is:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "P(y=1∣x)=σ(z)=\n",
        "1+e\n",
        "−z\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "where\n",
        "\n",
        "𝑧\n",
        "=\n",
        "𝑤\n",
        "⊤\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "=\n",
        "𝑤\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝑤\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑤\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "+\n",
        "𝑏\n",
        "z=w\n",
        "⊤\n",
        " x+b=w\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +w\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+w\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " +b\n",
        "𝑥\n",
        "=\n",
        "(\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        ")\n",
        "x=(x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,…,x\n",
        "n\n",
        "​\n",
        " ) is the vector of input features.\n",
        "\n",
        "𝑤\n",
        "=\n",
        "(\n",
        "𝑤\n",
        "1\n",
        ",\n",
        "𝑤\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑤\n",
        "𝑛\n",
        ")\n",
        "w=(w\n",
        "1\n",
        "​\n",
        " ,w\n",
        "2\n",
        "​\n",
        " ,…,w\n",
        "n\n",
        "​\n",
        " ) is the vector of weights.\n",
        "\n",
        "𝑏\n",
        "b is the bias (intercept) term.\n",
        "\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "σ(z) is the sigmoid function, which maps any real-valued number into the\n",
        "[\n",
        "0\n",
        ",\n",
        "1\n",
        "]\n",
        "[0,1] interval, interpretable as a probability.\n",
        "\n",
        "Interpretation\n",
        "The model outputs\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "P(y=1∣x).\n",
        "\n",
        "The probability of the other class (0) is\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "0\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "−\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "P(y=0∣x)=1−σ(z).\n",
        "\n",
        "The decision boundary is usually at\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "0.5\n",
        "σ(z)=0.5, or equivalently\n",
        "𝑧\n",
        "=\n",
        "0\n",
        "z=0.\n",
        "\n",
        "If you want the log-odds form (logit function), it's:\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "1\n",
        "−\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        ")\n",
        "=\n",
        "𝑤\n",
        "⊤\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "log(\n",
        "1−P(y=1∣x)\n",
        "P(y=1∣x)\n",
        "​\n",
        " )=w\n",
        "⊤\n",
        " x+b\n",
        "Let me know if you want me to explain how the model is trained or how it works in more detail!"
      ],
      "metadata": {
        "id": "BRQeXfci6yd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3 Why do we use the Sigmoid function in Logistic Regression?"
      ],
      "metadata": {
        "id": "7lhYMtVl6ya9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Output between 0 and 1:\n",
        "The Sigmoid function maps any real-valued number (from\n",
        "−\n",
        "∞\n",
        "−∞ to\n",
        "+\n",
        "∞\n",
        "+∞) into the range\n",
        "(\n",
        "0\n",
        ",\n",
        "1\n",
        ")\n",
        "(0,1). This is perfect for modeling probabilities, which must lie between 0 and 1.\n",
        "\n",
        "Probabilistic Interpretation:\n",
        "Logistic Regression is used for binary classification (e.g., yes/no, spam/not spam). The Sigmoid output can be interpreted as the probability that the input belongs to a particular class (usually class 1). For example, if Sigmoid outputs 0.8, it means an 80% chance the input is in class 1.\n",
        "\n",
        "Non-linear and Smooth:\n",
        "The Sigmoid function is smooth and differentiable everywhere, which makes it suitable for optimization using gradient descent. This smoothness helps the model converge more effectively.\n",
        "\n",
        "Decision Boundary:\n",
        "Using Sigmoid, you can set a threshold (commonly 0.5) to decide the predicted class. Values above 0.5 predict class 1, below 0.5 predict class 0."
      ],
      "metadata": {
        "id": "HagUzT0U6yX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4 What is the cost function of Logistic Regression?"
      ],
      "metadata": {
        "id": "3DegHTqQ6yU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->The cost function of Logistic Regression is called the Log Loss or Binary Cross-Entropy Loss.\n",
        "\n",
        "Explanation:\n",
        "Logistic Regression is used for binary classification, where the target\n",
        "𝑦\n",
        "y is either 0 or 1. The model predicts the probability\n",
        "𝑦\n",
        "^\n",
        "=\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "y\n",
        "^\n",
        "​\n",
        " =h\n",
        "θ\n",
        "​\n",
        " (x) that\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1, where\n",
        "\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝜎\n",
        "(\n",
        "𝜃\n",
        "𝑇\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝜃\n",
        "𝑇\n",
        "𝑥\n",
        "h\n",
        "θ\n",
        "​\n",
        " (x)=σ(θ\n",
        "T\n",
        " x)=\n",
        "1+e\n",
        "−θ\n",
        "T\n",
        " x\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "Here,\n",
        "𝜎\n",
        "(\n",
        "⋅\n",
        ")\n",
        "σ(⋅) is the sigmoid function.\n",
        "\n",
        "Cost function:\n",
        "For a single training example\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ",\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "(x\n",
        "(i)\n",
        " ,y\n",
        "(i)\n",
        " ), the cost is:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "(\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        ")\n",
        "J(θ)=−(y\n",
        "(i)\n",
        " log(h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))+(1−y\n",
        "(i)\n",
        " )log(1−h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " )))\n",
        "For the whole training set of\n",
        "𝑚\n",
        "m examples, the average cost function is:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "]\n",
        "J(θ)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " log(h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))+(1−y\n",
        "(i)\n",
        " )log(1−h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))]\n",
        "Intuition:\n",
        "If\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "=\n",
        "1\n",
        "y\n",
        "(i)\n",
        " =1, the cost is\n",
        "−\n",
        "log\n",
        "⁡\n",
        "(\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "−log(h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " )), which is low when the predicted probability is close to 1.\n",
        "\n",
        "If\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "=\n",
        "0\n",
        "y\n",
        "(i)\n",
        " =0, the cost is\n",
        "−\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "−log(1−h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " )), which is low when the predicted probability is close to 0.\n",
        "\n",
        "This encourages the model to assign high probabilities to the correct class.\n",
        "\n"
      ],
      "metadata": {
        "id": "MhRDQ6yf6yR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5 What is Regularization in Logistic Regression? Why is it needed?"
      ],
      "metadata": {
        "id": "RYUbBEkG6yO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty to the loss function of the model. Overfitting happens when a model learns not only the underlying pattern in the training data but also the noise, making it perform poorly on unseen data.\n",
        "\n",
        "Why is Regularization Needed?\n",
        "When training a logistic regression model, especially with a high number of features or limited data, the model might:\n",
        "\n",
        "Assign very large weights to certain features.\n",
        "\n",
        "Fit the training data too well, including noise.\n",
        "\n",
        "Perform poorly on test or real-world data (poor generalization).\n",
        "\n",
        "Regularization helps keep the model simpler and more generalizable by discouraging overly complex models.\n",
        "\n",
        "How Regularization Works\n",
        "In logistic regression, the goal is to minimize the loss function (log-loss or cross-entropy). Regularization modifies this loss function by adding a penalty term based on the model's weights.\n",
        "\n",
        "1. L2 Regularization (Ridge Regression)\n",
        "Adds the sum of the squares of the weights to the loss function:\n",
        "\n",
        "Loss\n",
        "=\n",
        "Log Loss\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑤\n",
        "𝑗\n",
        "2\n",
        "Loss=Log Loss+λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " w\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "Encourages smaller weights but doesn’t force them to zero.\n",
        "\n",
        "2. L1 Regularization (Lasso Regression)\n",
        "Adds the sum of the absolute values of the weights:\n",
        "\n",
        "Loss\n",
        "=\n",
        "Log Loss\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝑤\n",
        "𝑗\n",
        "∣\n",
        "Loss=Log Loss+λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣w\n",
        "j\n",
        "​\n",
        " ∣\n",
        "Can shrink some weights to exactly zero, helping with feature selection.\n",
        "\n",
        "3. Elastic Net Regularization\n",
        "Combines L1 and L2 penalties:\n",
        "\n",
        "Loss\n",
        "=\n",
        "Log Loss\n",
        "+\n",
        "𝜆\n",
        "1\n",
        "∑\n",
        "∣\n",
        "𝑤\n",
        "𝑗\n",
        "∣\n",
        "+\n",
        "𝜆\n",
        "2\n",
        "∑\n",
        "𝑤\n",
        "𝑗\n",
        "2\n",
        "Loss=Log Loss+λ\n",
        "1\n",
        "​\n",
        " ∑∣w\n",
        "j\n",
        "​\n",
        " ∣+λ\n",
        "2\n",
        "​\n",
        " ∑w\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "Key Parameters\n",
        "λ (lambda): Regularization strength. Higher values mean stronger regularization.\n",
        "\n",
        "In scikit-learn’s LogisticRegression, the parameter is C = 1/λ. So, smaller C = stronger regularization."
      ],
      "metadata": {
        "id": "OCFGPEY96yLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6 Explain the difference between Lasso, Ridge, and Elastic Net regression?"
      ],
      "metadata": {
        "id": "m4ZKPoez6yI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Lasso, Ridge, and Elastic Net are regularized regression techniques used to prevent overfitting in linear models by adding a penalty to the size of coefficients. They differ in the type of penalty applied:\n",
        "\n",
        "1. Ridge Regression (L2 Regularization)\n",
        "Penalty: Adds the sum of squared coefficients to the loss function.\n",
        "\n",
        "Loss\n",
        "=\n",
        "RSS\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑝\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "Loss=RSS+λ\n",
        "j=1\n",
        "∑\n",
        "p\n",
        "​\n",
        " β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "Effect:\n",
        "\n",
        "Shrinks coefficients toward zero but never exactly zero.\n",
        "\n",
        "Retains all features; useful when many features have small, but non-zero effects.\n",
        "\n",
        "Good for multicollinearity (highly correlated features).\n",
        "\n",
        "2. Lasso Regression (L1 Regularization)\n",
        "Penalty: Adds the sum of absolute values of coefficients to the loss function.\n",
        "\n",
        "Loss\n",
        "=\n",
        "RSS\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑝\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "Loss=RSS+λ\n",
        "j=1\n",
        "∑\n",
        "p\n",
        "​\n",
        " ∣β\n",
        "j\n",
        "​\n",
        " ∣\n",
        "Effect:\n",
        "\n",
        "Can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
        "\n",
        "Useful when we suspect that only a few variables are truly important.\n",
        "\n",
        "Can be unstable when features are highly correlated.\n",
        "\n",
        "3. Elastic Net Regression (L1 + L2 Regularization)\n",
        "Penalty: Combines both L1 and L2 penalties:\n",
        "\n",
        "Loss\n",
        "=\n",
        "RSS\n",
        "+\n",
        "𝜆\n",
        "1\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑝\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "+\n",
        "𝜆\n",
        "2\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑝\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "Loss=RSS+λ\n",
        "1\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "p\n",
        "​\n",
        " ∣β\n",
        "j\n",
        "​\n",
        " ∣+λ\n",
        "2\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "p\n",
        "​\n",
        " β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "Often rewritten as:\n",
        "\n",
        "Loss\n",
        "=\n",
        "RSS\n",
        "+\n",
        "𝜆\n",
        "(\n",
        "𝛼\n",
        "∑\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝛼\n",
        ")\n",
        "∑\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        ")\n",
        "Loss=RSS+λ(α∑∣β\n",
        "j\n",
        "​\n",
        " ∣+(1−α)∑β\n",
        "j\n",
        "2\n",
        "​\n",
        " )\n",
        "Where:\n",
        "\n",
        "𝜆\n",
        "λ controls overall regularization strength\n",
        "\n",
        "𝛼\n",
        "α controls the mix between Lasso and Ridge\n",
        "\n",
        "Effect:\n",
        "\n",
        "Balances between Ridge and Lasso.\n",
        "\n",
        "Performs well when features are correlated and some are irrelevant.\n",
        "\n",
        "Offers flexibility in model tuning."
      ],
      "metadata": {
        "id": "XGJzDEtc6yF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7 When should we use Elastic Net instead of Lasso or Ridge?"
      ],
      "metadata": {
        "id": "xjDcp-wt6yC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Elastic Net is a regularized regression technique that combines both Lasso (L1) and Ridge (L2) penalties. Choosing Elastic Net over just Lasso or Ridge is beneficial in certain scenarios. Here’s when you should consider using Elastic Net:\n",
        "\n",
        "🔧 Use Elastic Net when:\n",
        "1. You have many correlated features\n",
        "Lasso tends to pick one variable from a group of correlated ones and ignore the others.\n",
        "\n",
        "Ridge tends to spread the coefficients across correlated features.\n",
        "\n",
        "Elastic Net combines both behaviors, which can be advantageous. It can select groups of correlated variables together due to the L2 component, while still enforcing sparsity through the L1 component.\n",
        "\n",
        "Example: In genomics, where thousands of genes may be highly correlated, Elastic Net can give more stable and interpretable models.\n",
        "\n",
        "2. You want feature selection (like Lasso) but Lasso is too aggressive\n",
        "Lasso can zero out too many coefficients, especially when predictors are highly correlated or\n",
        "𝑝\n",
        ">\n",
        "𝑛\n",
        "p>n (more features than observations).\n",
        "\n",
        "Elastic Net softens the variable selection process by blending L1 and L2, often giving better performance and interpretability.\n",
        "\n",
        "3. Your data is high-dimensional (p >> n)\n",
        "In cases where the number of features far exceeds the number of observations, Ridge alone won't perform variable selection, and Lasso may be unstable or select too few features.\n",
        "\n",
        "Elastic Net handles high dimensionality more robustly by balancing L1 and L2 penalties.\n",
        "\n",
        "4. Model performance with Lasso or Ridge alone is suboptimal\n",
        "When you're unsure whether Lasso or Ridge is best, Elastic Net offers a compromise and often performs better through hyperparameter tuning (adjusting the mixing parameter\n",
        "𝛼\n",
        "α and the regularization strength\n",
        "𝜆\n",
        "λ)."
      ],
      "metadata": {
        "id": "zFhdiLep6x_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8 What is the impact of the regularization parameter (λ) in Logistic Regression?"
      ],
      "metadata": {
        "id": "XlzfH-OX6x9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->ChatGPT said:\n",
        "The regularization parameter (λ) in Logistic Regression plays a crucial role in controlling the complexity of the model and preventing overfitting. Here's a breakdown of its impact:\n",
        "\n",
        "🔧 What Regularization Does\n",
        "In logistic regression, regularization adds a penalty to the cost function to discourage the model from fitting the training data too closely (overfitting). The regularization term is controlled by the parameter λ (lambda).\n",
        "\n",
        "There are two common types of regularization:\n",
        "\n",
        "L2 Regularization (Ridge): Adds a penalty proportional to the square of the magnitude of the coefficients.\n",
        "\n",
        "L1 Regularization (Lasso): Adds a penalty proportional to the absolute value of the coefficients (can lead to sparse models).\n",
        "\n",
        "⚖️ Impact of λ (Lambda)\n",
        "Small λ (close to 0):\n",
        "\n",
        "The penalty term is minimal.\n",
        "\n",
        "The model has more freedom to fit the training data.\n",
        "\n",
        "Risk of overfitting increases.\n",
        "\n",
        "Coefficients can take on larger values.\n",
        "\n",
        "Large λ:\n",
        "\n",
        "The penalty term becomes dominant.\n",
        "\n",
        "The model is constrained and favors smaller coefficients.\n",
        "\n",
        "Risk of underfitting increases.\n",
        "\n",
        "May lead to simpler models with better generalization, especially when training data is limited or noisy.\n",
        "\n",
        "💡 Practical Insight\n",
        "In many libraries (like scikit-learn), instead of λ, a parameter C = 1/λ is used:\n",
        "\n",
        "High C → Low regularization (model can fit training data more closely).\n",
        "\n",
        "Low C → High regularization (more bias, less variance).\n",
        "\n",
        "📈 Example in Cost Function (L2 Regularization)\n",
        "If\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "J(θ) is the cost function:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "Logistic Loss\n",
        "+\n",
        "𝜆\n",
        "2\n",
        "𝑚\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜃\n",
        "𝑗\n",
        "2\n",
        "J(θ)=Logistic Loss+\n",
        "2m\n",
        "λ\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " θ\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑚\n",
        "m is the number of training examples.\n",
        "\n",
        "𝜃\n",
        "𝑗\n",
        "θ\n",
        "j\n",
        "​\n",
        "  are model parameters (excluding bias term)."
      ],
      "metadata": {
        "id": "Pen3jOcd6x6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9 What are the key assumptions of Logistic Regression?"
      ],
      "metadata": {
        "id": "O-iduZdl6x3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Logistic Regression is a powerful classification algorithm, but it relies on several key assumptions for optimal performance and valid interpretation. Here's a breakdown of those assumptions:\n",
        "\n",
        "🔑 Key Assumptions of Logistic Regression\n",
        "1. Binary (or Multinomial) Outcome\n",
        "Logistic regression assumes the dependent variable is categorical, typically:\n",
        "\n",
        "Binary (e.g., 0 or 1, yes or no)\n",
        "\n",
        "Can be extended to multinomial (more than two categories) using extensions like multinomial logistic regression.\n",
        "\n",
        "2. Linearity of Logit\n",
        "Logistic regression does not assume a linear relationship between the independent variables and the output.\n",
        "\n",
        "However, it does assume a linear relationship between the log-odds of the dependent variable and the independent variables:\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "log(\n",
        "1−p\n",
        "p\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        "\n",
        "This is known as the linearity in the logit assumption.\n",
        "\n",
        "3. Independence of Observations\n",
        "Observations should be independent of each other.\n",
        "\n",
        "This means no repeated measures or matched pairs unless using specialized models like conditional logistic regression.\n",
        "\n",
        "4. No or Little Multicollinearity\n",
        "Predictor variables should not be highly correlated with each other.\n",
        "\n",
        "High multicollinearity makes it difficult to estimate the coefficients accurately.\n",
        "\n",
        "Can be checked using Variance Inflation Factor (VIF).\n",
        "\n",
        "5. Large Sample Size\n",
        "Logistic regression requires a reasonably large sample size, especially when the outcome is rare.\n",
        "\n",
        "Small datasets may lead to overfitting and unreliable estimates.\n",
        "\n",
        "6. No Perfect Separation\n",
        "The model should not be able to perfectly separate the classes using the independent variables.\n",
        "\n",
        "If perfect separation occurs, logistic regression coefficients may become unstable or infinite.\n",
        "\n",
        "7. Independent Variables Are Not Strongly Influenced by Outliers\n",
        "Outliers in predictors can affect the model's stability.\n",
        "\n",
        "Logistic regression is sensitive to extreme values in continuous predictors.\n",
        "\n",
        "✅ Assumptions That Are Not Required\n",
        "Assumption\tRequired?\n",
        "Normality of predictors\t❌ No\n",
        "Homoscedasticity (equal variance)\t❌ No\n",
        "Errors are normally distributed\t❌ No"
      ],
      "metadata": {
        "id": "3vV-Au546x0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10 What are some alternatives to Logistic Regression for classification tasks?"
      ],
      "metadata": {
        "id": "MZzIxj666xx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->There are many alternatives to Logistic Regression for classification tasks, especially when dealing with complex data, non-linear relationships, or high-dimensional datasets. Here's a categorized overview of common alternatives:\n",
        "\n",
        "🔹 1. Linear Models (Like Logistic Regression, but with enhancements)\n",
        "Linear Discriminant Analysis (LDA)\n",
        "\n",
        "Assumes normal distribution and equal class covariances.\n",
        "\n",
        "Works well when these assumptions are met.\n",
        "\n",
        "Quadratic Discriminant Analysis (QDA)\n",
        "\n",
        "Similar to LDA, but allows different covariances per class.\n",
        "\n",
        "🔹 2. Tree-Based Methods\n",
        "Decision Trees\n",
        "\n",
        "Simple, interpretable, but prone to overfitting.\n",
        "\n",
        "Random Forest\n",
        "\n",
        "Ensemble of decision trees.\n",
        "\n",
        "Handles non-linearity, robust to overfitting, good for feature importance.\n",
        "\n",
        "Gradient Boosted Trees (e.g., XGBoost, LightGBM, CatBoost)\n",
        "\n",
        "Very powerful on tabular data.\n",
        "\n",
        "Often outperform logistic regression in accuracy.\n",
        "\n",
        "🔹 3. Support Vector Machines (SVM)\n",
        "Can handle non-linear decision boundaries using kernel tricks.\n",
        "\n",
        "Works well with high-dimensional data.\n",
        "\n",
        "Computationally intensive for large datasets.\n",
        "\n",
        "🔹 4. Neural Networks (Deep Learning)\n",
        "Multi-layer Perceptron (MLP)\n",
        "\n",
        "Good for capturing complex, non-linear relationships.\n",
        "\n",
        "Convolutional Neural Networks (CNNs)\n",
        "\n",
        "Excellent for image classification tasks.\n",
        "\n",
        "Recurrent Neural Networks (RNNs), LSTMs\n",
        "\n",
        "Suitable for sequential data (e.g., time series, text).\n",
        "\n",
        "🔹 5. Probabilistic Models\n",
        "Naive Bayes\n",
        "\n",
        "Fast, simple, effective for text classification.\n",
        "\n",
        "Based on strong independence assumptions.\n",
        "\n",
        "Bayesian Logistic Regression\n",
        "\n",
        "Probabilistic extension of logistic regression with Bayesian inference.\n",
        "\n",
        "🔹 6. K-Nearest Neighbors (KNN)\n",
        "Simple and non-parametric.\n",
        "\n",
        "Prediction based on closest training examples.\n",
        "\n",
        "Can be inefficient on large datasets or high-dimensional data.\n",
        "\n",
        "\n",
        "🔹 When to Choose Alternatives:\n",
        "Scenario\tAlternative\n",
        "Complex, non-linear relationships\tRandom Forest, XGBoost, Neural Networks\n",
        "Small, clean dataset\tLDA, QDA, SVM\n",
        "Need interpretability\tDecision Trees, Rule-Based Models\n",
        "High-dimensional sparse data (e.g., text)\tNaive Bayes, SVM\n",
        "Time-sensitive predictions\tLogistic Regression, Naive Bayes\n",
        "High predictive performance needed\tGradient Boosting, Neural Nets"
      ],
      "metadata": {
        "id": "XllPMcyu6xvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11 What are Classification Evaluation Metrics?"
      ],
      "metadata": {
        "id": "am7Uu8dK6xsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Classification evaluation metrics help you quantitatively assess the performance of a classification model. Different metrics provide insights into accuracy, error types, and prediction confidence, especially important when dealing with imbalanced datasets or high-stakes decisions.\n",
        "\n",
        "✅ Common Classification Evaluation Metrics\n",
        "1. Accuracy\n",
        "Definition: The ratio of correctly predicted observations to total observations.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Accuracy\n",
        "=\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝑇\n",
        "𝑁\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝑇\n",
        "𝑁\n",
        "+\n",
        "𝐹\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑁\n",
        "Accuracy=\n",
        "TP+TN+FP+FN\n",
        "TP+TN\n",
        "​\n",
        "\n",
        "Use when: Classes are balanced and the cost of false positives and false negatives is similar.\n",
        "\n",
        "2. Precision (Positive Predictive Value)\n",
        "Definition: How many predicted positives are actually positive.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Precision\n",
        "=\n",
        "𝑇\n",
        "𝑃\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑃\n",
        "Precision=\n",
        "TP+FP\n",
        "TP\n",
        "​\n",
        "\n",
        "Use when: False positives are costly, e.g., spam filters.\n",
        "\n",
        "3. Recall (Sensitivity or True Positive Rate)\n",
        "Definition: How many actual positives are captured.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Recall\n",
        "=\n",
        "𝑇\n",
        "𝑃\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑁\n",
        "Recall=\n",
        "TP+FN\n",
        "TP\n",
        "​\n",
        "\n",
        "Use when: False negatives are costly, e.g., disease detection.\n",
        "\n",
        "4. F1 Score\n",
        "Definition: Harmonic mean of precision and recall.\n",
        "\n",
        "Formula:\n",
        "\n",
        "F1\n",
        "=\n",
        "2\n",
        "⋅\n",
        "Precision\n",
        "⋅\n",
        "Recall\n",
        "Precision\n",
        "+\n",
        "Recall\n",
        "F1=2⋅\n",
        "Precision+Recall\n",
        "Precision⋅Recall\n",
        "​\n",
        "\n",
        "Use when: Balance between precision and recall is needed, especially on imbalanced datasets.\n",
        "\n",
        "5. ROC-AUC (Receiver Operating Characteristic - Area Under Curve)\n",
        "Definition: Measures the ability of the classifier to distinguish between classes across all thresholds.\n",
        "\n",
        "ROC Curve: Plot of True Positive Rate (Recall) vs. False Positive Rate.\n",
        "\n",
        "AUC Score: Higher = better separability.\n",
        "\n",
        "Use when: You want a threshold-independent performance measure.\n",
        "\n",
        "6. Confusion Matrix\n",
        "A table showing:\n",
        "\n",
        "mathematica\n",
        "Copy\n",
        "Edit\n",
        "|            | Predicted Positive | Predicted Negative |\n",
        "|------------|--------------------|--------------------|\n",
        "| Actual Pos |        TP          |        FN          |\n",
        "| Actual Neg |        FP          |        TN          |\n",
        "Useful for visualizing model errors and understanding class-specific performance.\n",
        "\n",
        "🧠 Choosing the Right Metric\n",
        "Scenario\tSuggested Metric(s)\n",
        "Balanced classes\tAccuracy\n",
        "Imbalanced classes\tPrecision, Recall, F1 Score\n",
        "High cost of false negatives\tRecall\n",
        "High cost of false positives\tPrecision\n",
        "Comparing models across thresholds\tROC-AUC, PR-AUC\n",
        "Probabilistic outputs matter\tLog Loss"
      ],
      "metadata": {
        "id": "uDLVbscA6xpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12 How does class imbalance affect Logistic Regression?"
      ],
      "metadata": {
        "id": "QDkssVNM6xmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Class imbalance can significantly affect Logistic Regression (and many other classifiers) by skewing the model's predictions toward the majority class. Here's a detailed breakdown of how and why this happens, along with ways to address it:\n",
        "\n",
        "🧠 How Class Imbalance Affects Logistic Regression\n",
        "1. Biased Decision Boundary\n",
        "Logistic Regression tries to maximize overall accuracy.\n",
        "\n",
        "With imbalanced data (e.g., 95% class A, 5% class B), predicting the majority class all the time can still give high accuracy (e.g., 95%).\n",
        "\n",
        "This leads to a biased model that performs poorly on the minority class.\n",
        "\n",
        "2. Poor Minority Class Recall (Sensitivity)\n",
        "The model may learn to ignore minority class patterns because the cost of misclassifying them (in the objective function) is low.\n",
        "\n",
        "As a result, recall for the minority class drops significantly.\n",
        "\n",
        "3. Skewed Probabilities\n",
        "The predicted probabilities can also be skewed toward the majority class, making threshold-based decisions (like using 0.5) less effective.\n",
        "\n",
        "🔧 Example\n",
        "Assume:\n",
        "\n",
        "950 examples of class 0 (negative)\n",
        "\n",
        "50 examples of class 1 (positive)\n",
        "\n",
        "A logistic regression model trained without addressing imbalance might:\n",
        "\n",
        "Predict almost all inputs as class 0\n",
        "\n",
        "Have ~95% accuracy\n",
        "\n",
        "But 0% recall for class 1 — it misses all the positives\n",
        "\n",
        "✅ Solutions to Handle Class Imbalance\n",
        "1. Resampling Techniques\n",
        "Oversampling the minority class (e.g., SMOTE)\n",
        "\n",
        "Undersampling the majority class\n",
        "\n",
        "Balances the dataset to help the model learn both classes equally\n",
        "\n",
        "2. Class Weights\n",
        "Modify the cost function to penalize misclassification of the minority class more heavily:\n",
        "\n",
        "In scikit-learn:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "LogisticRegression(class_weight='balanced')\n",
        "Or provide custom weights: class_weight={0: 1, 1: 10}\n",
        "\n",
        "3. Change Evaluation Metrics\n",
        "Instead of accuracy, use:\n",
        "\n",
        "Precision\n",
        "\n",
        "Recall\n",
        "\n",
        "F1-score\n",
        "\n",
        "ROC-AUC\n",
        "\n",
        "PR-AUC\n",
        "\n",
        "4. Threshold Adjustment\n",
        "Tune the classification threshold (default is 0.5) based on validation results to better balance precision and recall."
      ],
      "metadata": {
        "id": "7fWVa2NR6xjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13 What is Hyperparameter Tuning in Logistic Regression?"
      ],
      "metadata": {
        "id": "EmHP4FLR6xgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Hyperparameter Tuning in Logistic Regression refers to the process of selecting the optimal values for the model’s hyperparameters to improve its performance and generalization on unseen data.\n",
        "\n",
        "🔧 What Are Hyperparameters?\n",
        "In machine learning, hyperparameters are settings not learned from the data, but set before training the model. Unlike model parameters (like the weights in logistic regression), hyperparameters control how the model learns.\n",
        "\n",
        "⚙️ Common Hyperparameters in Logistic Regression\n",
        "When using libraries like scikit-learn (LogisticRegression), the key hyperparameters to tune are:\n",
        "\n",
        "Hyperparameter\tDescription\tEffect\n",
        "C\tInverse of regularization strength (C = 1/λ)\tSmaller C = stronger regularization\n",
        "penalty\tType of regularization ('l1', 'l2', 'elasticnet', or 'none')\tControls feature selection and model complexity\n",
        "solver\tOptimization algorithm ('liblinear', 'saga', etc.)\tNeeded for specific penalties (e.g., 'saga' for 'elasticnet')\n",
        "max_iter\tMaximum number of iterations\tEnsure convergence during training\n",
        "l1_ratio\tElastic net mixing parameter (between 0 and 1)\tUsed only with 'elasticnet' penalty\n",
        "\n",
        "🎯 Why Is Hyperparameter Tuning Important?\n",
        "Improves accuracy and generalization\n",
        "\n",
        "Prevents overfitting or underfitting\n",
        "\n",
        "Ensures stable convergence\n",
        "\n",
        "🔍 How to Perform Hyperparameter Tuning\n",
        "There are several standard techniques:\n",
        "\n",
        "1. Grid Search\n",
        "Tries all combinations of specified hyperparameter values.\n",
        "\n",
        "Can be computationally expensive.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # 'liblinear' supports both L1 and L2\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "print(grid.best_params_)\n",
        "2. Random Search\n",
        "Randomly selects combinations of parameters.\n",
        "\n",
        "More efficient than grid search when the space is large.\n",
        "\n",
        "3. Bayesian Optimization / Optuna / Hyperopt\n",
        "Advanced methods that model the performance of hyperparameters and intelligently search the space.\n",
        "\n",
        "Much more efficient for large datasets or high-dimensional tuning.\n",
        "\n",
        "✅ Best Practices\n",
        "Always use cross-validation (e.g., 5-fold) to evaluate combinations.\n",
        "\n",
        "Scale your features before tuning (especially important for regularization).\n",
        "\n",
        "Track performance metrics (accuracy, F1-score, AUC, etc.) during tuning.\n"
      ],
      "metadata": {
        "id": "sAUtu_FT6xdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14  What are different solvers in Logistic Regression? Which one should be used?"
      ],
      "metadata": {
        "id": "pVZNqMx16xar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Logistic Regression in libraries like Scikit-learn (sklearn.linear_model.LogisticRegression) supports different solvers, which are optimization algorithms used to minimize the loss function (log-loss). Each solver has strengths and ideal use cases depending on data size, regularization type, and sparsity.\n",
        "\n",
        "🔧 Common Solvers in Logistic Regression\n",
        "Solver\tOptimization Method\tSupports L1?\tSupports L2?\tSuitable for Large Data?\tSparse Data?\tMulticlass Support\n",
        "liblinear\tCoordinate Descent\t✅ (L1 & L2)\t✅\t❌ (small datasets)\t✅\tOne-vs-Rest only\n",
        "saga\tStochastic Average Gradient\t✅ (L1 & L2)\t✅\t✅\t✅\t✅ (native)\n",
        "lbfgs\tQuasi-Newton (BFGS variant)\t❌ (L2 only)\t✅\t✅\t❌\t✅ (native)\n",
        "newton-cg\tNewton-Raphson method\t❌ (L2 only)\t✅\t✅\t❌\t✅ (native)\n",
        "sag\tStochastic Average Gradient\t❌ (L2 only)\t✅\t✅\t✅\t✅ (native)\n",
        "\n",
        "⚖️ How to Choose the Right Solver\n",
        "✅ Use liblinear if:\n",
        "You need L1 regularization (sparse feature selection).\n",
        "\n",
        "Dataset is small or medium-sized.\n",
        "\n",
        "One-vs-rest multiclass strategy is acceptable.\n",
        "\n",
        "✅ Use saga if:\n",
        "Dataset is large or sparse.\n",
        "\n",
        "You need L1, L2, or Elastic Net regularization.\n",
        "\n",
        "You want true multiclass (softmax) support.\n",
        "\n",
        "It's the most versatile solver in sklearn.\n",
        "\n",
        "✅ Use lbfgs if:\n",
        "Dataset is medium to large.\n",
        "\n",
        "You're using L2 regularization.\n",
        "\n",
        "Data is dense.\n",
        "\n",
        "You need efficient multiclass support.\n",
        "\n",
        "✅ Use newton-cg if:\n",
        "You want better convergence on well-conditioned problems.\n",
        "\n",
        "Using L2 and don't mind slower computation.\n",
        "\n",
        "You need to handle multiclass problems.\n",
        "\n",
        "✅ Use sag if:\n",
        "Dataset is very large and features are sparse.\n",
        "\n",
        "You're okay with L2 only.\n",
        "\n",
        "🔍 Best Practice\n",
        "If unsure, start with:\n",
        "\n",
        "Small data: liblinear\n",
        "\n",
        "Large or sparse data: saga\n",
        "\n",
        "General purpose: lbfgs (especially with multi_class='multinomial')"
      ],
      "metadata": {
        "id": "8dL6VwmC6xYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15 How is Logistic Regression extended for multiclass classification?"
      ],
      "metadata": {
        "id": "On3W-cZw6xVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Logistic Regression is naturally a binary classification algorithm, but it can be extended to handle multiclass classification through two main techniques:\n",
        "\n",
        "1. One-vs-Rest (OvR) / One-vs-All (OvA)\n",
        "In the One-vs-Rest approach, the classifier trains one binary classifier for each class. Each classifier distinguishes one class from all other classes.\n",
        "\n",
        "For a dataset with\n",
        "𝑘\n",
        "k classes, you will train\n",
        "𝑘\n",
        "k binary classifiers:\n",
        "\n",
        "For each class, the classifier predicts whether the sample belongs to that class (positive) or not (negative).\n",
        "\n",
        "How It Works:\n",
        "The logistic regression model is trained\n",
        "𝑘\n",
        "k times, each time considering a single class as the target and all other classes as the negative class.\n",
        "\n",
        "When making predictions, each classifier outputs a probability for its class, and the class with the highest probability is selected as the final prediction.\n",
        "\n",
        "Example:\n",
        "For a dataset with 3 classes: Class A, Class B, and Class C, you'll train:\n",
        "\n",
        "Classifier 1: \"Is the sample from Class A or not?\"\n",
        "\n",
        "Classifier 2: \"Is the sample from Class B or not?\"\n",
        "\n",
        "Classifier 3: \"Is the sample from Class C or not?\"\n",
        "\n",
        "After training, the model will output the predicted probability for each class. The class with the highest probability will be chosen.\n",
        "\n",
        "Pros:\n",
        "Simple and easy to implement.\n",
        "\n",
        "Works well when the classes are not too imbalanced.\n",
        "\n",
        "Cons:\n",
        "If classes are highly imbalanced, the performance of the classifier may suffer because some classifiers may dominate the decision-making process.\n",
        "\n",
        "Can be computationally expensive when the number of classes is large.\n",
        "\n",
        "2. Softmax Regression (Multinomial Logistic Regression)\n",
        "In the Softmax Regression approach, logistic regression is directly extended to handle multiple classes by modifying the logistic function to a multiclass version.\n",
        "\n",
        "Instead of having one logistic function, we use the Softmax function to assign probabilities to each class, and the class with the highest probability is chosen.\n",
        "\n",
        "How It Works:\n",
        "The Softmax function is applied over the outputs of the linear function for each class. The softmax formula for class\n",
        "𝑖\n",
        "i is:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑖\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "𝑒\n",
        "𝑧\n",
        "𝑖\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "𝑒\n",
        "𝑧\n",
        "𝑗\n",
        "P(y=i∣X)=\n",
        "∑\n",
        "j=1\n",
        "k\n",
        "​\n",
        " e\n",
        "z\n",
        "j\n",
        "​\n",
        "\n",
        "\n",
        "e\n",
        "z\n",
        "i\n",
        "​\n",
        "\n",
        "\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑧\n",
        "𝑖\n",
        "=\n",
        "𝑤\n",
        "𝑖\n",
        "𝑇\n",
        "𝑋\n",
        "+\n",
        "𝑏\n",
        "𝑖\n",
        "z\n",
        "i\n",
        "​\n",
        " =w\n",
        "i\n",
        "T\n",
        "​\n",
        " X+b\n",
        "i\n",
        "​\n",
        "  is the score (linear combination of features) for class\n",
        "𝑖\n",
        "i,\n",
        "\n",
        "𝑘\n",
        "k is the total number of classes,\n",
        "\n",
        "The denominator normalizes the output so that the sum of the probabilities across all classes equals 1.\n",
        "\n",
        "During training, the model minimizes the cross-entropy loss between the true class labels and the predicted probabilities.\n",
        "\n",
        "Pros:\n",
        "More efficient than the One-vs-Rest approach.\n",
        "\n",
        "The model directly learns to predict probabilities for each class.\n",
        "\n",
        "Suitable when classes are mutually exclusive.\n",
        "\n",
        "Cons:\n",
        "Requires more computational resources than One-vs-Rest.\n",
        "\n",
        "Can be sensitive to class imbalances.\n",
        "\n",
        "Comparison of Both Methods\n",
        "Method\tDescription\tPros\tCons\n",
        "One-vs-Rest (OvR)\tOne binary classifier per class.\tSimpler, easy to implement, scalable.\tMay struggle with imbalanced classes, computationally expensive with many classes.\n",
        "Softmax Regression\tDirectly models the probability of each class.\tMore efficient, better for mutually exclusive classes.\tCan be harder to tune and more computationally expensive."
      ],
      "metadata": {
        "id": "fdWYYZec6xSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16 What are the advantages and disadvantages of Logistic Regression?"
      ],
      "metadata": {
        "id": "r73Hiqfx6xPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Advantages and Disadvantages of Logistic Regression\n",
        "\n",
        "Logistic Regression is one of the most popular and widely used machine learning algorithms, especially for binary classification problems. However, like all algorithms, it has its strengths and weaknesses. Below is a summary of its advantages and disadvantages:\n",
        "\n",
        "✅ Advantages of Logistic Regression\n",
        "Simplicity and Interpretability\n",
        "\n",
        "Easy to understand and implement: Logistic regression is straightforward to apply, especially for binary classification tasks.\n",
        "\n",
        "Interpretability: The coefficients of the model (i.e., weights) directly show the relationship between each feature and the outcome. For example, a positive coefficient means that as the feature increases, the probability of the positive class increases.\n",
        "\n",
        "Efficient and Fast\n",
        "\n",
        "Computationally efficient: Logistic regression is relatively fast to train, especially on smaller datasets. Its simplicity makes it less computationally intensive compared to more complex models like neural networks.\n",
        "\n",
        "Requires fewer resources: It doesn’t require significant computational power (e.g., GPUs) to train.\n",
        "\n",
        "Works Well for Linearly Separable Data\n",
        "\n",
        "Good for linear decision boundaries: If the data can be separated by a linear decision boundary, logistic regression performs very well. This is often the case in many real-world problems.\n",
        "\n",
        "\n",
        "❌ Disadvantages of Logistic Regression\n",
        "Linearity Assumption\n",
        "\n",
        "Assumes linear relationships: Logistic regression assumes that the relationship between the independent variables (features) and the log-odds of the dependent variable is linear. This can be limiting for more complex, non-linear relationships in the data.\n",
        "\n",
        "Poor Performance on Complex Problems\n",
        "\n",
        "Struggles with non-linear decision boundaries: Logistic regression may perform poorly if the decision boundary between classes is highly non-linear or complicated, as it cannot capture these patterns. In such cases, more complex models like Decision Trees or Neural Networks are often more effective.\n",
        "\n",
        "Sensitive to Feature Scaling\n",
        "\n",
        "Requires feature scaling: Logistic regression works better when the features are scaled (normalized or standardized). Features with vastly different scales can impact the performance and convergence of the algorithm, making it necessary to preprocess the data properly.\n",
        "\n",
        "Not Robust to Outliers\n",
        "\n",
        "Sensitive to outliers: Logistic regression is sensitive to outliers, as it tries to fit the best decision boundary that minimizes the cost function. Extreme outliers can distort the decision boundary and lead to poor performance.\n",
        "\n",
        "Limited to Binary Classification (in its basic form)\n",
        "\n",
        "While extensions like Multinomial Logistic Regression exist for multi-class classification, the standard form of logistic regression is inherently binary. This can be restrictive when working with multi-class problems unless you adapt the model appropriately.\n",
        "\n"
      ],
      "metadata": {
        "id": "L477By6-6xMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17 What are some use cases of Logistic Regression?"
      ],
      "metadata": {
        "id": "bGJD59yQ6w3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Logistic Regression is a widely used algorithm in machine learning due to its simplicity and efficiency, especially for classification tasks. Below are several common use cases of logistic regression across various fields:\n",
        "\n",
        "1. Medical Diagnosis\n",
        "Problem: Predicting whether a patient has a certain disease based on medical data.\n",
        "\n",
        "Example: Predicting whether a patient has diabetes or not based on factors like age, BMI, glucose levels, etc.\n",
        "\n",
        "Outcome: Binary classification (0 = no disease, 1 = disease present).\n",
        "\n",
        "2. Credit Scoring and Risk Assessment\n",
        "Problem: Assessing the likelihood that a loan applicant will default on a loan.\n",
        "\n",
        "Example: Using factors like income, credit history, debt, and employment status to predict whether a borrower will default (1) or repay (0).\n",
        "\n",
        "Outcome: Binary classification (0 = safe loan, 1 = risky loan).\n",
        "\n",
        "3. Spam Email Detection\n",
        "Problem: Classifying emails as either spam or ham (non-spam).\n",
        "\n",
        "Example: Using features like the presence of certain keywords, sender email address, and email subject to predict if an email is spam.\n",
        "\n",
        "Outcome: Binary classification (0 = non-spam, 1 = spam).\n",
        "\n",
        "4. Customer Churn Prediction\n",
        "Problem: Predicting whether a customer will cancel a subscription to a service.\n",
        "\n",
        "Example: Analyzing customer behaviors like usage patterns, customer support interaction, and payment history to predict churn.\n",
        "\n",
        "Outcome: Binary classification (0 = will not churn, 1 = will churn).\n",
        "\n",
        "5. Fraud Detection\n",
        "Problem: Identifying whether a transaction is fraudulent or legitimate.\n",
        "\n",
        "Example: In financial transactions, using transaction amount, time of transaction, location, and user behavior to detect fraud.\n",
        "\n",
        "Outcome: Binary classification (0 = legitimate transaction, 1 = fraudulent transaction).\n",
        "\n",
        "6. Market Research (Purchase Prediction)\n",
        "Problem: Predicting the likelihood of a customer purchasing a product based on demographic or behavioral data.\n",
        "\n",
        "Example: Using data on past purchases, browsing behavior, and customer demographics to predict whether a customer will buy a product.\n",
        "\n",
        "Outcome: Binary classification (0 = will not buy, 1 = will buy).\n",
        "\n",
        "7. Disease Propagation (Epidemiology)\n",
        "Problem: Predicting the likelihood of disease transmission or outbreak.\n",
        "\n",
        "Example: Using data like population density, healthcare accessibility, vaccination rates, and historical outbreak patterns to predict the likelihood of an outbreak.\n",
        "\n",
        "Outcome: Binary classification (0 = no outbreak, 1 = outbreak).\n",
        "\n"
      ],
      "metadata": {
        "id": "X2qxBitQ6w0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18 What is the difference between Softmax Regression and Logistic Regression?"
      ],
      "metadata": {
        "id": "8S0v569F6wxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Both Logistic Regression and Softmax Regression (also known as Multinomial Logistic Regression) are classification algorithms that predict class probabilities. However, they differ in terms of the number of classes they handle and the output they generate.\n",
        "\n",
        "1. Target Problem Type\n",
        "Logistic Regression:\n",
        "\n",
        "Binary classification problem (i.e., two classes).\n",
        "\n",
        "The model outputs the probability that a sample belongs to one of two classes (usually coded as 0 or 1).\n",
        "\n",
        "For a given input\n",
        "𝑥\n",
        "x, the model predicts:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝑤\n",
        "𝑇\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "P(y=1∣x)=\n",
        "1+e\n",
        "−(w\n",
        "T\n",
        " x+b)\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "The output is a probability value between 0 and 1 for the positive class (class 1), and the probability of the negative class (class 0) is simply\n",
        "1\n",
        "−\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "1−P(y=1∣x).\n",
        "\n",
        "Softmax Regression (Multinomial Logistic Regression):\n",
        "\n",
        "Multiclass classification problem (i.e., more than two classes).\n",
        "\n",
        "The model predicts the probabilities for multiple classes (e.g., class 1, class 2, … class\n",
        "𝐶\n",
        "C).\n",
        "\n",
        "The softmax function generalizes the logistic function for multi-class problems. For\n",
        "𝐶\n",
        "C classes, the model predicts the probability for each class:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑐\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝑒\n",
        "𝑤\n",
        "𝑐\n",
        "𝑇\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "𝑐\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝐶\n",
        "𝑒\n",
        "𝑤\n",
        "𝑖\n",
        "𝑇\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "𝑖\n",
        "P(y=c∣x)=\n",
        "∑\n",
        "i=1\n",
        "C\n",
        "​\n",
        " e\n",
        "w\n",
        "i\n",
        "T\n",
        "​\n",
        " x+b\n",
        "i\n",
        "​\n",
        "\n",
        "\n",
        "e\n",
        "w\n",
        "c\n",
        "T\n",
        "​\n",
        " x+b\n",
        "c\n",
        "​\n",
        "\n",
        "\n",
        "​\n",
        "\n",
        "The output is a vector of probabilities for each class, and the sum of all probabilities equals 1.\n",
        "\n",
        "2. Output\n",
        "Logistic Regression:\n",
        "\n",
        "Produces a single probability for the positive class (class 1), with the negative class probability inferred as\n",
        "1\n",
        "−\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "1−P(y=1∣x).\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "Produces a probability distribution over multiple classes. For\n",
        "𝐶\n",
        "C classes, the output is a vector of\n",
        "𝐶\n",
        "C probabilities, where each probability corresponds to the likelihood of the sample belonging to one of the classes.\n",
        "\n",
        "3. Number of Classes\n",
        "Logistic Regression:\n",
        "\n",
        "Used for binary classification (two classes).\n",
        "\n",
        "Can be extended to multiple classes using One-vs-Rest or One-vs-One approaches.\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "Specifically designed for multiclass classification.\n",
        "\n",
        "Directly handles multiple classes without needing additional techniques like One-vs-Rest.\n",
        "\n",
        "4. Decision Boundaries\n",
        "Logistic Regression:\n",
        "\n",
        "The decision boundary is a single line (or hyperplane in higher dimensions) that separates the two classes.\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "In the case of multiclass problems, the decision boundaries are more complex, as the model involves multiple hyperplanes that divide the feature space based on the class probabilities.\n",
        "\n",
        "5. Cost Function\n",
        "Logistic Regression:\n",
        "\n",
        "The loss function used is binary cross-entropy (also called log loss) for two classes:\n",
        "\n",
        "𝐿\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "[\n",
        "𝑦\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "]\n",
        "L=−\n",
        "N\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "N\n",
        "​\n",
        " [y\n",
        "i\n",
        "​\n",
        " log(\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " )+(1−y\n",
        "i\n",
        "​\n",
        " )log(1−\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " )]\n",
        "Softmax Regression:\n",
        "\n",
        "The loss function is categorical cross-entropy for multiclass problems:\n",
        "\n",
        "𝐿\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "𝑐\n",
        "=\n",
        "1\n",
        "𝐶\n",
        "𝑦\n",
        "𝑖\n",
        ",\n",
        "𝑐\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ",\n",
        "𝑐\n",
        ")\n",
        "L=−\n",
        "N\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "N\n",
        "​\n",
        "  \n",
        "c=1\n",
        "∑\n",
        "C\n",
        "​\n",
        " y\n",
        "i,c\n",
        "​\n",
        " log(\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i,c\n",
        "​\n",
        " )\n",
        "Where\n",
        "𝑦\n",
        "𝑖\n",
        ",\n",
        "𝑐\n",
        "y\n",
        "i,c\n",
        "​\n",
        "  is a one-hot encoded vector, and\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ",\n",
        "𝑐\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i,c\n",
        "​\n",
        "  is the predicted probability for class\n",
        "𝑐\n",
        "c.\n",
        "\n",
        "6. Applications\n",
        "Logistic Regression:\n",
        "\n",
        "Used when you have a binary outcome or two classes.\n",
        "\n",
        "Examples: Spam detection (spam or not spam), disease detection (sick or healthy).\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "Used for multiclass problems, where there are more than two classes.\n",
        "\n",
        "Examples: Image classification (cat, dog, horse), digit recognition (0–9), text classification (topics, genres)."
      ],
      "metadata": {
        "id": "s7YXQTvy6wuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19  How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?"
      ],
      "metadata": {
        "id": "QG4FfNP_6wnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->ChatGPT said:\n",
        "Choosing between One-vs-Rest (OvR) and Softmax (Multinomial) for multiclass classification depends on several factors, such as your dataset, model requirements, computational constraints, and performance goals. Let’s break down when and why you might choose one over the other:\n",
        "\n",
        "1. Model Complexity and Efficiency\n",
        "One-vs-Rest (OvR):\n",
        "\n",
        "Simple and intuitive. This method trains one binary classifier per class, so it can be seen as a collection of multiple binary classification problems.\n",
        "\n",
        "Scalability: It can be easier to scale when you have many classes because each classifier is independent.\n",
        "\n",
        "Computation: When the number of classes\n",
        "𝑘\n",
        "k is large, OvR can be more computationally efficient because you train multiple simpler binary classifiers (each only considering one class vs. all others).\n",
        "\n",
        "Softmax (Multinomial):\n",
        "\n",
        "Single classifier: Instead of multiple classifiers, Softmax directly computes the probability for each class using a single model.\n",
        "\n",
        "Efficiency: While training a multinomial logistic regression with Softmax might require more computational resources (since it involves solving for all classes at once), it is more efficient in terms of prediction, as only one classifier is needed at inference time.\n",
        "\n",
        "When to choose:\n",
        "Choose OvR if you want simpler, more interpretable models or need to scale to many classes. OvR is also easier to implement and less computationally expensive for datasets with few features.\n",
        "\n",
        "Choose Softmax when you want a single model to directly output probabilities for each class, and you can afford the slightly higher computational cost during training.\n",
        "\n",
        "2. Class Imbalance\n",
        "One-vs-Rest (OvR):\n",
        "\n",
        "Sensitivity to imbalanced data: Since each OvR classifier only focuses on one class, it can be more sensitive to class imbalance. For example, if a class is underrepresented, the classifier for that class may struggle to perform well.\n",
        "\n",
        "Fixes: You can handle class imbalance by using techniques like class weighting or resampling in OvR.\n",
        "\n",
        "Softmax (Multinomial):\n",
        "\n",
        "The multinomial Softmax model tends to be more robust to imbalanced classes because it considers all classes simultaneously and incorporates the relationships between classes, but it can still be sensitive to very severe imbalance.\n",
        "\n",
        "When to choose:\n",
        "Choose Softmax when you have moderate class imbalance because it accounts for all classes simultaneously and may give better results for imbalanced data.\n",
        "\n",
        "If your classes are highly imbalanced and you choose OvR, you'll need to address imbalance in each classifier (e.g., by adjusting class weights or using sampling strategies).\n",
        "\n",
        "3. Output Interpretation\n",
        "One-vs-Rest (OvR):\n",
        "\n",
        "Each classifier outputs a binary decision (whether an instance belongs to the positive class or not).\n",
        "\n",
        "During prediction, you get multiple probability scores from each classifier, and you pick the class with the highest probability.\n",
        "\n",
        "Interpretability: It's easier to understand the model's decision since each binary classifier focuses on one class.\n",
        "\n",
        "Softmax (Multinomial):\n",
        "\n",
        "The output is a probability distribution over all classes, and each instance is assigned a probability for each class.\n",
        "\n",
        "This can be a more natural representation for problems where the classes are mutually exclusive, and you want to model the relative probabilities of each class.\n",
        "\n",
        "When to choose:\n",
        "Choose OvR if you prefer binary decision boundaries and simpler, more interpretable outputs.\n",
        "\n",
        "Choose Softmax when you need a probabilistic interpretation of class membership, or if the classes are mutually exclusive and you want to account for relative probabilities.\n",
        "\n",
        "4. Multi-Class Confusion\n",
        "One-vs-Rest (OvR):\n",
        "\n",
        "In the OvR strategy, each classifier learns to distinguish a particular class from the rest. This can lead to problems if your classes are not linearly separable, as the separate classifiers might not capture interactions between classes well.\n",
        "\n",
        "If the decision boundaries between classes are overlapping or complex, OvR might fail to generalize well.\n",
        "\n",
        "Softmax (Multinomial):\n",
        "\n",
        "The Softmax model directly learns to distinguish between all classes simultaneously. As a result, it’s more likely to handle complex decision boundaries between classes. It also better models the relationships between classes, which can be crucial for more sophisticated decision-making.\n",
        "\n",
        "When to choose:\n",
        "Choose Softmax if your classes have complex or non-linear decision boundaries, as it directly models these relationships.\n",
        "\n",
        "Choose OvR when you need to handle simpler, more separable classes.\n",
        "\n",
        "5. Number of Classes\n",
        "One-vs-Rest (OvR):\n",
        "\n",
        "When you have many classes (e.g., hundreds or thousands), OvR can sometimes be more efficient because it treats each class independently, which might be easier to manage in practice.\n",
        "\n",
        "However, as the number of classes increases, the model will require more time and resources for training since you’ll have to train one classifier per class.\n",
        "\n",
        "Softmax (Multinomial):\n",
        "\n",
        "Multinomial Softmax will need to calculate a probability for each class in a single pass, which may be more efficient in terms of prediction. However, training time increases with more classes due to the need to estimate parameters for each class simultaneously.\n",
        "\n",
        "When to choose:\n",
        "Choose OvR if you have a very large number of classes and you want the flexibility to train each class separately.\n",
        "\n",
        "Choose Softmax when you h"
      ],
      "metadata": {
        "id": "MezUoYvK7Is9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20  How do we interpret coefficients in Logistic Regression?"
      ],
      "metadata": {
        "id": "1yyTAzOZ7Ios"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Interpreting the coefficients in Logistic Regression helps us understand the relationship between the independent variables (features) and the probability of the outcome (target variable). The coefficients are essential for understanding the influence of each feature on the model’s predictions.\n",
        "\n",
        "Here’s how to interpret the coefficients in Logistic Regression:\n",
        "\n",
        "1. Logistic Regression Model\n",
        "In a simple binary logistic regression model, the log-odds of the dependent variable\n",
        "𝑌\n",
        "Y being 1 (i.e., the event occurring) are modeled as a linear combination of the independent variables\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑋\n",
        "𝑛\n",
        "X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        " ,…,X\n",
        "n\n",
        "​\n",
        " :\n",
        "\n",
        "logit\n",
        "(\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        ")\n",
        "=\n",
        "ln\n",
        "⁡\n",
        "(\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "1\n",
        "−\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "logit(P(Y=1∣X))=ln(\n",
        "1−P(Y=1∣X)\n",
        "P(Y=1∣X)\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "P(Y=1∣X) is the probability of the positive class (event occurring).\n",
        "\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        "  is the intercept.\n",
        "\n",
        "𝛽\n",
        "1\n",
        ",\n",
        "𝛽\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝛽\n",
        "𝑛\n",
        "β\n",
        "1\n",
        "​\n",
        " ,β\n",
        "2\n",
        "​\n",
        " ,…,β\n",
        "n\n",
        "​\n",
        "  are the coefficients of the independent variables.\n",
        "\n",
        "The logistic function (sigmoid) then maps the log-odds to a probability between 0 and 1:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        ")\n",
        "P(Y=1∣X)=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "2. Interpretation of Coefficients\n",
        "A. Sign of the Coefficients\n",
        "Positive Coefficient (\n",
        "𝛽\n",
        "𝑖\n",
        ">\n",
        "0\n",
        "β\n",
        "i\n",
        "​\n",
        " >0): A positive coefficient means that as the feature\n",
        "𝑋\n",
        "𝑖\n",
        "X\n",
        "i\n",
        "​\n",
        "  increases, the probability of the outcome being 1 (positive class) increases. In other words, higher values of the feature make the outcome more likely.\n",
        "\n",
        "Negative Coefficient (\n",
        "𝛽\n",
        "𝑖\n",
        "<\n",
        "0\n",
        "β\n",
        "i\n",
        "​\n",
        " <0): A negative coefficient means that as the feature\n",
        "𝑋\n",
        "𝑖\n",
        "X\n",
        "i\n",
        "​\n",
        "  increases, the probability of the outcome being 1 decreases. In other words, higher values of the feature make the outcome less likely.\n",
        "\n",
        "B. Magnitude of the Coefficients\n",
        "The magnitude of a coefficient represents how much the log-odds of the outcome change with a one-unit change in the feature.\n",
        "\n",
        "Larger magnitudes mean that the feature has a stronger influence on the outcome, while smaller magnitudes mean a weaker influence.\n",
        "\n",
        "C. Odds Ratio (Interpretation in terms of odds)\n",
        "The coefficient\n",
        "𝛽\n",
        "𝑖\n",
        "β\n",
        "i\n",
        "​\n",
        "  is interpreted in terms of odds ratios. The odds ratio gives us a multiplicative factor by which the odds of the outcome change for a one-unit increase in the corresponding feature.\n",
        "\n",
        "Exponentiating the coefficient (i.e.,\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        "e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        " ) gives the odds ratio for that feature:\n",
        "\n",
        "Odds Ratio\n",
        "=\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        "Odds Ratio=e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        "\n",
        "This tells you how the odds of the outcome change when the corresponding feature increases by one unit.\n",
        "\n",
        "If\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        ">\n",
        "1\n",
        "e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        " >1: The odds of the event happening (outcome = 1) increase when\n",
        "𝑋\n",
        "𝑖\n",
        "X\n",
        "i\n",
        "​\n",
        "  increases.\n",
        "\n",
        "If\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        "<\n",
        "1\n",
        "e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        " <1: The odds of the event happening (outcome = 1) decrease when\n",
        "𝑋\n",
        "𝑖\n",
        "X\n",
        "i\n",
        "​\n",
        "  increases.\n",
        "\n",
        "If\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        " =1: The feature has no effect on the odds of the event.\n",
        "\n",
        "Example:\n",
        "Let’s consider a logistic regression model predicting whether a person will buy a product (1) or not (0) based on their age.\n",
        "\n",
        "Suppose the model output is:\n",
        "\n",
        "logit\n",
        "(\n",
        "𝑃\n",
        ")\n",
        "=\n",
        "−\n",
        "4.0\n",
        "+\n",
        "0.05\n",
        "×\n",
        "age\n",
        "logit(P)=−4.0+0.05×age\n",
        "Intercept (\n",
        "𝛽\n",
        "0\n",
        "=\n",
        "−\n",
        "4.0\n",
        "β\n",
        "0\n",
        "​\n",
        " =−4.0): When age = 0, the log-odds of the person buying the product is -4. This doesn’t have much practical meaning (as age = 0 is not realistic), but it helps us calculate the probability of the outcome when age is 0.\n",
        "\n",
        "Coefficient (\n",
        "𝛽\n",
        "1\n",
        "=\n",
        "0.05\n",
        "β\n",
        "1\n",
        "​\n",
        " =0.05): For every 1-year increase in age, the log-odds of buying the product increase by 0.05. This suggests that older individuals are slightly more likely to purchase the product.\n",
        "\n",
        "Odds Ratio: To find the odds ratio for age:\n",
        "\n",
        "𝑒\n",
        "0.05\n",
        "≈\n",
        "1.051\n",
        "e\n",
        "0.05\n",
        " ≈1.051\n",
        "This means that for each additional year of age, the odds of buying the product increase by a factor of 1.051. In other words, with each year of age, a person’s likelihood of buying the product increases by about 5.1%.\n",
        "\n",
        "3. Odds and Probability Relationship\n",
        "Log-Odds to Probability: The logistic regression output gives you log-odds, but for practical interpretation, we often convert them to probabilities (values between 0 and 1).\n",
        "\n",
        "To convert the log-odds to probabilities:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        ")\n",
        "P(Y=1∣X)=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "This conversion allows us to understand how likely an event is to occur based on the input features.\n",
        "\n",
        "4. Example Interpretation:\n",
        "Suppose we have a logistic regression model to predict whether a patient has a disease (1 = Yes, 0 = No) based on two features: age and BMI.\n",
        "\n",
        "Model equation:\n",
        "\n",
        "logit\n",
        "(\n",
        "𝑃\n",
        ")\n",
        "=\n",
        "−\n",
        "3.5\n",
        "+\n",
        "0.03\n",
        "×\n",
        "age\n",
        "+\n",
        "0.2\n",
        "×\n",
        "BMI\n",
        "logit(P)=−3.5+0.03×age+0.2×BMI\n",
        "Intercept (\n",
        "−\n",
        "3.5\n",
        "−3.5): The log-odds of having the disease when both age = 0 and BMI = 0 is -3.5 (again, not practical for real-world interpretation, but mathematically necessary).\n",
        "\n",
        "Age Coefficient (0.03): For every one-year increase in age, the log-odds of having the disease increase by 0.03. The odds ratio for age is\n",
        "𝑒\n",
        "0.03\n",
        "≈\n",
        "1.031\n",
        "e\n",
        "0.03\n",
        " ≈1.031, meaning the odds of having the disease increase by 3.1% for each additional year of age.\n",
        "\n",
        "BMI Coefficient (0.2): For every one-unit increase in BMI, the log-odds of having the disease increase by 0.2. The odds ratio for BMI is\n",
        "𝑒\n",
        "0.2\n",
        "≈\n",
        "1.221\n",
        "e\n",
        "0.2\n",
        " ≈1.221, meaning the odds of having the disease increase by 22.1% for each additional unit of BMI."
      ],
      "metadata": {
        "id": "JTn1Hz9V7Lnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical"
      ],
      "metadata": {
        "id": "XxU28Fuy7bBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1 Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and  prints the model accuracy.\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load a sample dataset (Iris dataset in this case)\n",
        "# You can replace this with your own dataset by using pd.read_csv or other methods\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2GV0R3J7vmz",
        "outputId": "12355c02-87bc-451f-ccc4-5ee6dab3887f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load a sample dataset (Iris dataset in this case)\n",
        "# You can replace this with your own dataset by using pd.read_csv or other methods\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy with L1 regularization (Lasso): {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKt8KNOw7wKE",
        "outputId": "cd7f43bb-435e-4cb5-bf7b-382f4db00f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy with L1 regularization (Lasso): 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load a sample dataset (Iris dataset in this case)\n",
        "# You can replace this with your own dataset by using pd.read_csv or other methods\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy with L2 regularization (Ridge): {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the coefficients learned by the model\n",
        "print(\"\\nModel Coefficients (weights):\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WWJtwIe71SB",
        "outputId": "c6337ad4-f612-4735-81e4-529e01da53ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy with L2 regularization (Ridge): 100.00%\n",
            "\n",
            "Model Coefficients (weights):\n",
            "[[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n",
            " [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n",
            " [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load a sample dataset (Iris dataset in this case)\n",
        "# You can replace this with your own dataset by using pd.read_csv or other methods\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with Elastic Net regularization\n",
        "# ElasticNet requires both L1 ratio and penalty set to 'elasticnet'\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy with Elastic Net regularization: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the coefficients learned by the model\n",
        "print(\"\\nModel Coefficients (weights):\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxgOpM-RACmj",
        "outputId": "980eda97-9eea-4f2e-d3ea-ac8fe91ea046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy with Elastic Net regularization: 100.00%\n",
            "\n",
            "Model Coefficients (weights):\n",
            "[[ 0.38921786  1.77119399 -2.42199518 -0.70797951]\n",
            " [ 0.07768231  0.          0.         -0.58153362]\n",
            " [-1.259546   -1.52887518  2.59702975  2.07844203]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5 Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'C.\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset (multiclass classification problem)\n",
        "data = load_iris()\n",
        "X = data.data  # Features (sepal and petal lengths and widths)\n",
        "y = data.target  # Target (class labels: 0, 1, or 2)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model for multiclass classification using OvR strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy for multiclass classification (One-vs-Rest): {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the coefficients learned by the model\n",
        "print(\"\\nModel Coefficients (weights) for each class:\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8yXTqtQAMqj",
        "outputId": "42b191b8-b523-4f83-a970-78a2d220325e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy for multiclass classification (One-vs-Rest): 100.00%\n",
            "\n",
            "Model Coefficients (weights) for each class:\n",
            "[[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n",
            " [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n",
            " [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6 Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2']  # Regularization type (L1 or L2)\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Train the model using GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
        "\n",
        "# Evaluate the model on the test set using the best found parameters\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Print the accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with the best parameters: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUWLJE1172_L",
        "outputId": "f22c5cab-a763-425c-a0af-cc288b2d8021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found: {'C': 10, 'penalty': 'l1'}\n",
            "Accuracy with the best parameters: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7 Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# Define Stratified K-Fold Cross-Validation with 5 folds\n",
        "strat_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# List to store accuracy scores for each fold\n",
        "accuracy_scores = []\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "for train_index, test_index in strat_kfold.split(X, y):\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model on the training set\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate and store the accuracy for this fold\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "# Calculate and print the average accuracy across all folds\n",
        "average_accuracy = np.mean(accuracy_scores)\n",
        "print(f\"Average accuracy from Stratified K-Fold Cross-Validation: {average_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpHdyD_A73X7",
        "outputId": "8870dfe2-f59b-4585-8526-fdf49b067d28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy from Stratified K-Fold Cross-Validation: 96.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8 Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data.drop('target_column_name', axis=1)\n",
        "y = data['target_column_name']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "CDvc-mP-73-M",
        "outputId": "1c20b303-07fa-47a4-c23b-54a66cd4c682",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6d212d023c8f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'your_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Separate features (X) and target variable (y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9 Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# 1. Load and Prepare Data\n",
        "# Replace with your actual data loading and preprocessing\n",
        "# For demonstration, we'll use a synthetic dataset\n",
        "# Create a synthetic dataset with 2 features and 1 target (binary)\n",
        "X = np.random.rand(100, 2)\n",
        "y = np.random.randint(0, 2, 100)  # Binary target (0 or 1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Define Hyperparameter Space\n",
        "# Define the range of values for each hyperparameter to be tuned\n",
        "param_distributions = {\n",
        "    'C': uniform.rvs(scale=100, size=50),  # Inverse of regularization strength (C=1/lambda)\n",
        "    'penalty': ['l1', 'l2'],  # Regularization type\n",
        "    'solver': ['liblinear', 'lbfgs', 'sag', 'saga'],  # Algorithm to use in the optimization\n",
        "    # Note: 'liblinear' is compatible with l1 regularization, while 'sag' and 'saga' are compatible with l2 and l1\n",
        "}\n",
        "\n",
        "# 3. Initialize Logistic Regression and Randomized Search\n",
        "# Create a Logistic Regression model\n",
        "logreg = LogisticRegression(random_state=42)\n",
        "# Create a RandomizedSearchCV object to perform the hyperparameter tuning\n",
        "random_search = RandomizedSearchCV(logreg,\n",
        "                                   param_distributions=param_distributions,\n",
        "                                   scoring='accuracy',  # Accuracy metric for evaluation\n",
        "                                   n_iter=10,  # Number of random combinations to try\n",
        "                                   cv=5,       # 5-fold cross-validation\n",
        "                                   verbose=1)  # Show progress during search\n",
        "\n",
        "# 4. Perform Hyperparameter Tuning\n",
        "# Fit the RandomizedSearchCV object to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# 5. Print Best Parameters and Accuracy\n",
        "# Get the best parameters found during the search\n",
        "best_params = random_search.best_params_\n",
        "print(\"Best parameters:\", best_params)\n",
        "\n",
        "# Get the best estimator (model with the best hyperparameters)\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# 6. Evaluate the Model\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "# Calculate the accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy on test set:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5QMlZTX74WD",
        "outputId": "f1804977-005e-4557-e45d-7396cb479431"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best parameters: {'solver': 'saga', 'penalty': 'l2', 'C': np.float64(24.943952783297483)}\n",
            "Accuracy on test set: 0.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "25 fits failed out of a total of 50.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [  nan 0.475 0.475   nan   nan 0.475 0.475 0.475   nan   nan]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10 Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target labels (multiclass)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "log_reg = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# Wrap the Logistic Regression model with One-vs-One classifier\n",
        "ovo_classifier = OneVsOneClassifier(log_reg)\n",
        "\n",
        "# Train the model using the training data\n",
        "ovo_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = ovo_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy (One-vs-One): {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOB7ETp775Lb",
        "outputId": "512a193d-8539-453c-d662-ff54f90f0bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy (One-vs-One): 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11 Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load a binary classification dataset (e.g., Iris dataset or custom dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset and consider only two classes for binary classification\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Filter out all but two classes (0 and 1 for binary classification)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using Seaborn\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "dlwrAxsu76C7",
        "outputId": "d7119915-d7c8-4448-a4cb-b2db0e6a7f03"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12  Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Since this is a multiclass dataset, we need to focus on binary classification.\n",
        "# For simplicity, we will only take classes 0 and 1 for this example.\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the performance metrics\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "# Optionally, print the full classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MQVx8ov76YV",
        "outputId": "6201471f-799e-46fa-e9da-49c3dff51aa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-Score: 1.00\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        12\n",
            "           1       1.00      1.00      1.00         8\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13 Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate an imbalanced binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with class_weight='balanced'\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print classification report (Precision, Recall, F1-Score)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Generate and visualize the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix using seaborn heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "aPhLmoa7767L",
        "outputId": "9c5b10f8-c4a7-4ec9-b25d-b886f542f1c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.84      0.90       185\n",
            "           1       0.25      0.67      0.36        15\n",
            "\n",
            "    accuracy                           0.82       200\n",
            "   macro avg       0.61      0.75      0.63       200\n",
            "weighted avg       0.91      0.82      0.86       200\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14 Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic_df = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"First few rows of the Titanic dataset:\")\n",
        "print(titanic_df.head())\n",
        "\n",
        "# Handle missing values\n",
        "# Using SimpleImputer to fill missing values\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "titanic_df['Age'] = imputer.fit_transform(titanic_df[['Age']])\n",
        "\n",
        "# Dropping the \"Cabin\" column since it's missing too many values\n",
        "titanic_df = titanic_df.drop(columns=['Cabin'])\n",
        "\n",
        "# Encoding categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encoding \"Sex\" (male=0, female=1)\n",
        "titanic_df['Sex'] = label_encoder.fit_transform(titanic_df['Sex'])\n",
        "\n",
        "# Encoding \"Embarked\" (C=0, Q=1, S=2)\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Handling missing values in \"Embarked\" (use most frequent value)\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0])\n",
        "\n",
        "# Select features (X) and target variable (y)\n",
        "X = titanic_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "y = titanic_df['Survived']\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TE3ic0t77PL",
        "outputId": "58d73c19-4129-4e11-a099-f44d7c4d5493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the Titanic dataset:\n",
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n",
            "Accuracy: 79.89%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.83       105\n",
            "           1       0.78      0.72      0.75        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.80      0.79      0.79       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15 Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic_df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "titanic_df['Age'] = imputer.fit_transform(titanic_df[['Age']])\n",
        "\n",
        "# Drop the \"Cabin\" column since it's missing too many values\n",
        "titanic_df = titanic_df.drop(columns=['Cabin'])\n",
        "\n",
        "# Encoding categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "titanic_df['Sex'] = label_encoder.fit_transform(titanic_df['Sex'])\n",
        "\n",
        "# Encoding \"Embarked\" (C=0, Q=1, S=2)\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Handling missing values in \"Embarked\"\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0])\n",
        "\n",
        "# Select features (X) and target variable (y)\n",
        "X = titanic_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "y = titanic_df['Survived']\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model 1: Logistic Regression without Scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model without scaling\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "\n",
        "# Evaluate model performance without scaling\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling * 100:.2f}%\")\n",
        "\n",
        "# Print classification report for model without scaling\n",
        "print(\"\\nClassification Report (No Scaling):\")\n",
        "print(classification_report(y_test, y_pred_no_scaling))\n",
        "\n",
        "# Model 2: Logistic Regression with Scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Standardize the features (train data)\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model_with_scaling = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model with scaling\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Standardize the test data\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Make predictions on the scaled test data\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate model performance with scaling\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "print(f\"\\nAccuracy with scaling: {accuracy_with_scaling * 100:.2f}%\")\n",
        "\n",
        "# Print classification report for model with scaling\n",
        "print(\"\\nClassification Report (With Scaling):\")\n",
        "print(classification_report(y_test, y_pred_with_scaling))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJiWyq_v77q8",
        "outputId": "058c9e34-c9df-47f9-f198-48c58eef8ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 79.89%\n",
            "\n",
            "Classification Report (No Scaling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.83       105\n",
            "           1       0.78      0.72      0.75        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.80      0.79      0.79       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n",
            "\n",
            "Accuracy with scaling: 79.89%\n",
            "\n",
            "Classification Report (With Scaling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.83       105\n",
            "           1       0.78      0.72      0.75        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.80      0.79      0.79       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16 Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic_df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "titanic_df['Age'] = imputer.fit_transform(titanic_df[['Age']])\n",
        "\n",
        "# Drop the \"Cabin\" column since it's missing too many values\n",
        "titanic_df = titanic_df.drop(columns=['Cabin'])\n",
        "\n",
        "# Encoding categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "titanic_df['Sex'] = label_encoder.fit_transform(titanic_df['Sex'])\n",
        "\n",
        "# Encoding \"Embarked\" (C=0, Q=1, S=2)\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Handling missing values in \"Embarked\"\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0])\n",
        "\n",
        "# Select features (X) and target variable (y)\n",
        "X = titanic_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "y = titanic_df['Survived']\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make probability predictions on the test data\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='green', label='ROC Curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line (random classifier)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "-L3MmxzG77_r",
        "outputId": "5d9f13f5-a679-41f4-d34e-639aa97acb1f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.8835\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17 Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic_df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "titanic_df['Age'] = imputer.fit_transform(titanic_df[['Age']])\n",
        "\n",
        "# Drop the \"Cabin\" column since it's missing too many values\n",
        "titanic_df = titanic_df.drop(columns=['Cabin'])\n",
        "\n",
        "# Encoding categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "titanic_df['Sex'] = label_encoder.fit_transform(titanic_df['Sex'])\n",
        "\n",
        "# Encoding \"Embarked\" (C=0, Q=1, S=2)\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Handling missing values in \"Embarked\"\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0])\n",
        "\n",
        "# Select features (X) and target variable (y)\n",
        "X = titanic_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "y = titanic_df['Survived']\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with C=0.5 (custom regularization)\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy with custom learning rate (C=0.5): {accuracy * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO9wak4578Yj",
        "outputId": "639e3086-13c2-41a4-b9eb-2bde1698e689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with custom learning rate (C=0.5): 80.45%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18 Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic_df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "titanic_df['Age'] = imputer.fit_transform(titanic_df[['Age']])\n",
        "\n",
        "# Drop the \"Cabin\" column since it's missing too many values\n",
        "titanic_df = titanic_df.drop(columns=['Cabin'])\n",
        "\n",
        "# Encoding categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "titanic_df['Sex'] = label_encoder.fit_transform(titanic_df['Sex'])\n",
        "\n",
        "# Encoding \"Embarked\" (C=0, Q=1, S=2)\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Handling missing values in \"Embarked\"\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0])\n",
        "\n",
        "# Select features (X) and target variable (y)\n",
        "X = titanic_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "y = titanic_df['Survived']\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get the model coefficients\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Create a DataFrame to display the feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': coefficients,\n",
        "    'Absolute Coefficient': abs(coefficients)\n",
        "})\n",
        "\n",
        "# Sort the features by the absolute value of the coefficients\n",
        "feature_importance = feature_importance.sort_values(by='Absolute Coefficient', ascending=False)\n",
        "\n",
        "# Print the important features\n",
        "print(\"Important Features based on model coefficients:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Optional: Plot the feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance['Feature'], feature_importance['Absolute Coefficient'], color='blue')\n",
        "plt.xlabel('Absolute Coefficient Value')\n",
        "plt.title('Feature Importance Based on Logistic Regression Coefficients')\n",
        "plt.gca().invert_yaxis()  # Reverse the order to display the most important features at the top\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "-lRmjZ4B79JM",
        "outputId": "5c844347-ef89-40fb-bb89-74dcb61543b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Important Features based on model coefficients:\n",
            "    Feature  Coefficient  Absolute Coefficient\n",
            "1       Sex    -2.596117              2.596117\n",
            "0    Pclass    -0.924209              0.924209\n",
            "3     SibSp    -0.288731              0.288731\n",
            "6  Embarked    -0.209031              0.209031\n",
            "4     Parch    -0.102082              0.102082\n",
            "2       Age    -0.028036              0.028036\n",
            "5      Fare     0.002605              0.002605\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19 Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic_df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "titanic_df['Age'] = imputer.fit_transform(titanic_df[['Age']])\n",
        "\n",
        "# Drop the \"Cabin\" column since it's missing too many values\n",
        "titanic_df = titanic_df.drop(columns=['Cabin'])\n",
        "\n",
        "# Encoding categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "titanic_df['Sex'] = label_encoder.fit_transform(titanic_df['Sex'])\n",
        "\n",
        "# Encoding \"Embarked\" (C=0, Q=1, S=2)\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Handling missing values in \"Embarked\"\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0])\n",
        "\n",
        "# Select features (X) and target variable (y)\n",
        "X = titanic_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "y = titanic_df['Survived']\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Cohen's Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "# Print the Cohen's Kappa Score\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwDj71jN791s",
        "outputId": "26313916-1c69-4615-955d-7cb752e0045d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.5803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20 Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic_df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "titanic_df['Age'] = imputer.fit_transform(titanic_df[['Age']])\n",
        "\n",
        "# Drop the \"Cabin\" column since it's missing too many values\n",
        "titanic_df = titanic_df.drop(columns=['Cabin'])\n",
        "\n",
        "# Encoding categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "titanic_df['Sex'] = label_encoder.fit_transform(titanic_df['Sex'])\n",
        "\n",
        "# Encoding \"Embarked\" (C=0, Q=1, S=2)\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Handling missing values in \"Embarked\"\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0])\n",
        "\n",
        "# Select features (X) and target variable (y)\n",
        "X = titanic_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "y = titanic_df['Survived']\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities for the positive class (Survived = 1)\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision and recall for different thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Compute the area under the Precision-Recall curve\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='orange', label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "NQP2vX5K7-J0",
        "outputId": "fe47d9e7-70fd-434e-ec66-b97f01404e53"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21 Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic_df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "titanic_df['Age'] = imputer.fit_transform(titanic_df[['Age']])\n",
        "\n",
        "# Drop the \"Cabin\" column since it's missing too many values\n",
        "titanic_df = titanic_df.drop(columns=['Cabin'])\n",
        "\n",
        "# Encoding categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "titanic_df['Sex'] = label_encoder.fit_transform(titanic_df['Sex'])\n",
        "\n",
        "# Encoding \"Embarked\" (C=0, Q=1, S=2)\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Handling missing values in \"Embarked\"\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0])\n",
        "\n",
        "# Select features (X) and target variable (y)\n",
        "X = titanic_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "y = titanic_df['Survived']\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Initialize a dictionary to store the accuracy results\n",
        "accuracy_results = {}\n",
        "\n",
        "# Train Logistic Regression using each solver and compute accuracy\n",
        "for solver in solvers:\n",
        "    # Initialize the Logistic Regression model with the specified solver\n",
        "    model = LogisticRegression(solver=solver, max_iter=200, random_state=42)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Store the results\n",
        "    accuracy_results[solver] = accuracy\n",
        "\n",
        "# Print the accuracy results for each solver\n",
        "print(\"Accuracy comparison for different solvers:\")\n",
        "for solver, accuracy in accuracy_results.items():\n",
        "    print(f\"Solver: {solver}, Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX1LlFcj7-cE",
        "outputId": "2da7c867-83f3-4d64-f5d9-8342bc202ead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy comparison for different solvers:\n",
            "Solver: liblinear, Accuracy: 0.7821\n",
            "Solver: saga, Accuracy: 0.6704\n",
            "Solver: lbfgs, Accuracy: 0.7989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22 Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic_df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "titanic_df['Age'] = imputer.fit_transform(titanic_df[['Age']])\n",
        "\n",
        "# Drop the \"Cabin\" column since it's missing too many values\n",
        "titanic_df = titanic_df.drop(columns=['Cabin'])\n",
        "\n",
        "# Encoding categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "titanic_df['Sex'] = label_encoder.fit_transform(titanic_df['Sex'])\n",
        "\n",
        "# Encoding \"Embarked\" (C=0, Q=1, S=2)\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Handling missing values in \"Embarked\"\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0])\n",
        "\n",
        "# Select features (X) and target variable (y)\n",
        "X = titanic_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "y = titanic_df['Survived']\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Print the MCC score\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDsJqr1B7-yL",
        "outputId": "b1c94efa-d9b3-4b37-964b-e85e2c9961f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.5817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23 Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic_df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "titanic_df['Age'] = imputer.fit_transform(titanic_df[['Age']])\n",
        "\n",
        "# Drop the \"Cabin\" column since it's missing too many values\n",
        "titanic_df = titanic_df.drop(columns=['Cabin'])\n",
        "\n",
        "# Encoding categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "titanic_df['Sex'] = label_encoder.fit_transform(titanic_df['Sex'])\n",
        "\n",
        "# Encoding \"Embarked\" (C=0, Q=1, S=2)\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Handling missing values in \"Embarked\"\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0])\n",
        "\n",
        "# Select features (X) and target variable (y)\n",
        "X = titanic_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "y = titanic_df['Survived']\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train on the raw data\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_raw = model.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train on the standardized data\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print the accuracy comparison\n",
        "print(f\"Accuracy on raw data: {accuracy_raw:.4f}\")\n",
        "print(f\"Accuracy on standardized data: {accuracy_scaled:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksSZt6Wc7_RN",
        "outputId": "3a7cc663-bb37-4aeb-99dd-3df0b0dbdebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data: 0.7989\n",
            "Accuracy on standardized data: 0.7989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24 Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic_df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "titanic_df['Age'] = imputer.fit_transform(titanic_df[['Age']])\n",
        "\n",
        "# Drop the \"Cabin\" column since it's missing too many values\n",
        "titanic_df = titanic_df.drop(columns=['Cabin'])\n",
        "\n",
        "# Encoding categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "titanic_df['Sex'] = label_encoder.fit_transform(titanic_df['Sex'])\n",
        "\n",
        "# Encoding \"Embarked\" (C=0, Q=1, S=2)\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Handling missing values in \"Embarked\"\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0])\n",
        "\n",
        "# Select features (X) and target variable (y)\n",
        "X = titanic_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "y = titanic_df['Survived']\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Set up the hyperparameter grid for C\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Use GridSearchCV to search for the optimal C value using cross-validation\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best value of C\n",
        "best_C = grid_search.best_params_['C']\n",
        "print(f\"Best value of C: {best_C}\")\n",
        "\n",
        "# Evaluate the model with the best C on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy with the optimal C: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQlSrJAC7_ik",
        "outputId": "704ca18b-6dea-4734-956e-9f5e7643da0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best value of C: 1\n",
            "Accuracy with the optimal C: 0.7989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ta6GkcUm6qD5",
        "outputId": "6b086cab-5702-470c-f491-21d8993b9808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the loaded model: 0.7989\n"
          ]
        }
      ],
      "source": [
        "#25 Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic_df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "titanic_df['Age'] = imputer.fit_transform(titanic_df[['Age']])\n",
        "\n",
        "# Drop the \"Cabin\" column since it's missing too many values\n",
        "titanic_df = titanic_df.drop(columns=['Cabin'])\n",
        "\n",
        "# Encoding categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "titanic_df['Sex'] = label_encoder.fit_transform(titanic_df['Sex'])\n",
        "\n",
        "# Encoding \"Embarked\" (C=0, Q=1, S=2)\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Handling missing values in \"Embarked\"\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0])\n",
        "\n",
        "# Select features (X) and target variable (y)\n",
        "X = titanic_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "y = titanic_df['Survived']\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model using joblib\n",
        "joblib.dump(model, 'logistic_regression_model.pkl')\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load('logistic_regression_model.pkl')\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy of the loaded model\n",
        "print(f\"Accuracy of the loaded model: {accuracy:.4f}\")\n"
      ]
    }
  ]
}